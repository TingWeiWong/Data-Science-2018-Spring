{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia2014 + Gigaword5 and GloVe\n",
    "ref: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.Session(config = config)\n",
    "\n",
    "from keras.datasets import reuters\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Input, Conv1D, MaxPooling1D, LSTM, Flatten, Dense, Dropout, Reshape, Bidirectional, Conv2D, MaxPool2D, Concatenate, Activation\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_NB_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "VALIDATION_SPLIT = 0.3\n",
    "#############################################################################\n",
    "# TODO: this should be the same as the dimension of word embedding you are  #\n",
    "# using                                                                     #\n",
    "#############################################################################\n",
    "EMBEDDING_DIM = None\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# TODO: please assign the absolute path to the GloVe txt file               #\n",
    "# (glove.6B.50d.txt or any dimension you like)                              #\n",
    "#############################################################################\n",
    "path_to_glove = None\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################\n",
    "tmp_file_name = 'word2vec.6B.txt'\n",
    "\n",
    "glove_file = datapath(path_to_glove)\n",
    "tmp_file = get_tmpfile(tmp_file_name)\n",
    "\n",
    "glove2word2vec(glove_file, tmp_file)\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the type the word vector of 'animal', the dimension (shape) of this vector, and the content of thie vector.\n",
    "\n",
    "If you use 300 dimensions word vectors, the correct outout will be something like this:\n",
    "```\n",
    "(numpy.ndarray,\n",
    " (300,),\n",
    " array([ 0.25653  ,  0.66592  , -0.5313   ,  0.20342  ,  0.40049  ,\n",
    " ... multiple lines are omitted ...\n",
    "        -0.14921  ,  0.2404   ,  0.22182  ,  0.68883  , -0.018991 ],\n",
    "       dtype=float32))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# TODO: please print the type the word vector of 'animal', the dimension    #\n",
    "# (shape) of this vector, and the content of thie vector.                   #\n",
    "#############################################################################\n",
    "\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [similar_by_vector](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.similar_by_vector) to obtain the 10 most similar word to 'france'\n",
    "\n",
    "If you use 300 dimensions word vectors, the correct outout will be something like this:\n",
    "```\n",
    "[('france', 1.0),\n",
    " ('french', 0.7344760894775391),\n",
    " ('paris', 0.6580672264099121),\n",
    " ('belgium', 0.620672345161438),\n",
    " ('spain', 0.573593258857727),\n",
    " ('italy', 0.5643459558486938),\n",
    " ('germany', 0.5567397475242615),\n",
    " ('prohertrib', 0.5564222931861877),\n",
    " ('britain', 0.5553334951400757),\n",
    " ('chirac', 0.5362644195556641)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# TODO: use similar_by_vector to obtain the 10 most similar word to 'france'#\n",
    "#############################################################################\n",
    "\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [most_similar](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Word2VecKeyedVectors.most_similar) to obtain the result of 'woman' + 'king' - 'man'\n",
    "\n",
    "If you use 300 dimensions, the correct output will be somthing like this:\n",
    "```\n",
    "[('queen', 0.6713277101516724),\n",
    " ('princess', 0.5432624220848083),\n",
    " ('throne', 0.5386104583740234),\n",
    " ('monarch', 0.5347574949264526),\n",
    " ('daughter', 0.498025119304657),\n",
    " ('mother', 0.4956442713737488),\n",
    " ('elizabeth', 0.4832652509212494),\n",
    " ('kingdom', 0.47747087478637695),\n",
    " ('prince', 0.4668239951133728),\n",
    " ('wife', 0.4647327661514282)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# TODO: use most_similar to obtain the result of 'woman' + 'king' - 'man'   #\n",
    "#############################################################################\n",
    "\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [doesnt_match](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.doesnt_match) to obtain the word with the different meaning with the other words in a list 'breakfast', 'lunch', 'dinner', 'cereal'\n",
    "\n",
    "If you use 300 dimensions, the correct output will be something like this:\n",
    "```\n",
    "'cereal'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# TODO: Use doesnt_match to obtain the word with the different meaning with #\n",
    "# the other words in a list 'breakfast', 'lunch', 'dinner', 'cereal'        #\n",
    "#############################################################################\n",
    "\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pca_plot(model):\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = PCA(n_components = 2)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    for i in range(len(x)):\n",
    "        label = labels[i]\n",
    "        if label in ['king', 'queen', 'sister', 'brother', 'niece', 'nephew', 'aunt', 'uncle', 'woman', 'man', \n",
    "                     'madam', 'sir']:\n",
    "            plt.scatter(x[i],y[i])\n",
    "            plt.annotate(label,\n",
    "                         xy = (x[i], y[i]),\n",
    "                         xytext = (5, 2),\n",
    "                         textcoords = 'offset points',\n",
    "                         ha = 'right',\n",
    "                         va = 'bottom')\n",
    "    plt.show()\n",
    "\n",
    "pca_plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reuter-21578 Data set with word2vec\n",
    "Last week, you have already worked on Reuters-21578 dataset for multi-class classification. This week, you are using word2vec to classify the same dataset.\n",
    "\n",
    "In this lab, you will have to implement 3 three neural network models using keras API:\n",
    "1. Mutilayer perceptron\n",
    "2. Conv1D\n",
    "3. LSTM or GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this requires download from the first time\n",
    "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words = MAX_NB_WORDS, \n",
    "                                                         skip_top = 0, \n",
    "                                                         maxlen = MAX_SEQUENCE_LENGTH,\n",
    "                                                         seed = 113,\n",
    "                                                         start_char = 1, \n",
    "                                                         oov_char = 2, \n",
    "                                                         index_from = 3)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "word2index = reuters.get_word_index()\n",
    "word2index = {key : (value + 3) for (key, value) in word2index.items()}\n",
    "word2index['<PAD>'] = 0\n",
    "word2index['<START>'] = 1\n",
    "word2index['<UNK>'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# TODO: create a embeddings_index to map word to word vector and print the  #\n",
    "# number of words in the embedding                                          #\n",
    "#############################################################################\n",
    "pass\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "# TODO: create a embedding_matrix that map the index of word2index to word  #\n",
    "# vectors                                                                   #\n",
    "#############################################################################\n",
    "pass\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################\n",
    "\n",
    "#############################################################################\n",
    "# TODO: define an embedding layer that initialize the weights with          #                                         \n",
    "# embedding_matrix and set trainable to False                               #\n",
    "#############################################################################\n",
    "pass\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################\n",
    "embedding_layer = Embedding(len(word2index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights = [embedding_matrix],\n",
    "                            input_length = MAX_SEQUENCE_LENGTH,\n",
    "                            trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# TODO: Use keras pad_sequences to pad the sequence in X_train and X_test   #\n",
    "# to MAX_SEQUENCE_LENGTH                                                    #\n",
    "#############################################################################\n",
    "pass\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Shape of X_train tensor:', X_train.shape)\n",
    "print('Shape of X_test tensor:', X_test.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X_train = X_train[indices]\n",
    "y_train = y_train[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * X_train.shape[0])\n",
    "\n",
    "X_val = X_train[-nb_validation_samples:]\n",
    "y_val = y_train[-nb_validation_samples:]\n",
    "X_train = X_train[:-nb_validation_samples]\n",
    "y_train = y_train[:-nb_validation_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec + multiple layer perceptron\n",
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_1 (InputLayer)         (None, 300)               0         \n",
    "_________________________________________________________________\n",
    "embedding_1 (Embedding)      (None, 300, 300)          9294900   \n",
    "_________________________________________________________________\n",
    "flatten_1 (Flatten)          (None, 90000)             0         \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 128)               11520128  \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 128)               0         \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 64)                8256      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 64)                0         \n",
    "_________________________________________________________________\n",
    "dense_3 (Dense)              (None, 32)                2080      \n",
    "_________________________________________________________________\n",
    "activation_3 (Activation)    (None, 32)                0         \n",
    "_________________________________________________________________\n",
    "dense_4 (Dense)              (None, 46)                1518      \n",
    "_________________________________________________________________\n",
    "activation_4 (Activation)    (None, 46)                0         \n",
    "=================================================================\n",
    "Total params: 20,826,882\n",
    "Trainable params: 11,531,982\n",
    "Non-trainable params: 9,294,900\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# TODO: Define the model of multiple layer perceptron similar to the model  #\n",
    "# summary above into the variable model, compile it and fit it.             #\n",
    "#############################################################################\n",
    "pass\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec + Conv1D\n",
    "```\n",
    "_________________________________________________________________\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_2 (InputLayer)         (None, 300)               0         \n",
    "_________________________________________________________________\n",
    "embedding_1 (Embedding)      (None, 300, 300)          9294900   \n",
    "_________________________________________________________________\n",
    "conv1d_1 (Conv1D)            (None, 296, 64)           96064     \n",
    "_________________________________________________________________\n",
    "max_pooling1d_1 (MaxPooling1 (None, 59, 64)            0         \n",
    "_________________________________________________________________\n",
    "flatten_2 (Flatten)          (None, 3776)              0         \n",
    "_________________________________________________________________\n",
    "dropout_1 (Dropout)          (None, 3776)              0         \n",
    "_________________________________________________________________\n",
    "dense_5 (Dense)              (None, 64)                241728    \n",
    "_________________________________________________________________\n",
    "dropout_2 (Dropout)          (None, 64)                0         \n",
    "_________________________________________________________________\n",
    "dense_6 (Dense)              (None, 46)                2990      \n",
    "=================================================================\n",
    "Total params: 9,635,682\n",
    "Trainable params: 340,782\n",
    "Non-trainable params: 9,294,900\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# TODO: Define the model of 1D convolution similar to the model summary     #\n",
    "# above into the variable model, compile it and fit it.                     #\n",
    "#############################################################################\n",
    "pass\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec + LSTM (GRU)\n",
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_3 (InputLayer)         (None, 300)               0         \n",
    "_________________________________________________________________\n",
    "embedding_1 (Embedding)      (None, 300, 300)          9294900   \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, 128)               219648    \n",
    "_________________________________________________________________\n",
    "dense_7 (Dense)              (None, 46)                5934      \n",
    "=================================================================\n",
    "Total params: 9,520,482\n",
    "Trainable params: 225,582\n",
    "Non-trainable params: 9,294,900\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_input = Input(shape = (MAX_SEQUENCE_LENGTH,), dtype = 'int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "units = 128\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "layer1 = LSTM(units,\n",
    "    dropout = 0.2,\n",
    "    recurrent_dropout = 0.2,\n",
    "    return_sequences = False)\n",
    "x = layer1(embedded_sequences)\n",
    "\n",
    "final_layer = Dense(46, activation = 'softmax')\n",
    "preds = final_layer(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "print(model.summary())\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['acc'])\n",
    "\n",
    "model.fit(X_train, y_train, validation_data = (X_val, y_val),\n",
    "          epochs = 20, batch_size = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
