{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import time\n",
    "from random import randrange\n",
    "from past.builtins import xrange\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train original shape (60000, 28, 28)\n",
      "y_train original shape (60000,)\n",
      "X_test original shape (10000, 28, 28)\n",
      "y_test original shape (10000,)\n",
      "class\tcount\n",
      "0\t5923\n",
      "1\t6742\n",
      "2\t5958\n",
      "3\t6131\n",
      "4\t5842\n",
      "5\t5421\n",
      "6\t5918\n",
      "7\t6265\n",
      "8\t5851\n",
      "9\t5949\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWUAAAEYCAYAAACEOeekAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a5ad7d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_classes = 10\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#############################################################################\n",
    "# TODO: Print the shape of the training data and testing data               #\n",
    "# Plot the previous 9 training data and title their class                   #\n",
    "# Count the number of data for each class in training data                  #\n",
    "#############################################################################\n",
    "print(\"X_train original shape {}\".format(X_train.shape))\n",
    "print(\"y_train original shape {}\".format(y_train.shape))\n",
    "print(\"X_test original shape {}\".format(X_test.shape))\n",
    "print(\"y_test original shape {}\".format(y_test.shape))\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(X_train[i], cmap = 'gray', interpolation = 'none')\n",
    "    plt.title(\"Class {}\".format(y_train[i]))\n",
    "plt.tight_layout()\n",
    "    \n",
    "unique, count = np.unique(y_train, return_counts = True)\n",
    "cls_count = np.concatenate((unique.reshape(nb_classes, 1), count.reshape(nb_classes, 1)), axis = 1)\n",
    "print('class\\tcount')\n",
    "print('\\n'.join(['{}\\t{}'.format(item[0], item[1]) for item in cls_count]))\n",
    "# for cls in cls_count:\n",
    "#     print('{}\\t{}'.format(str(cls[0]), str(cls[1])))\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (6742, 785)\n",
      "Train label shape: (6742,)\n",
      "Validation data shape: (6742, 785)\n",
      "Validation label shape: (6742,)\n",
      "Test data shape: (2270, 785)\n",
      "Test label shape: (2270,)\n"
     ]
    }
   ],
   "source": [
    "# data prepration for binary classification\n",
    "not_n = 1\n",
    "def get_mnist_data(X_train, y_train, X_test, y_test):\n",
    "    # normalization\n",
    "    X_train = X_train.reshape((X_train.shape[0], -1)) / 255.\n",
    "    X_test = X_test.reshape((X_test.shape[0], -1)) / 255.\n",
    "    \n",
    "    # binarize\n",
    "    y_train[y_train != not_n] = 0\n",
    "    y_train[y_train == not_n] = 1\n",
    "    y_test[y_test != not_n] = 0\n",
    "    y_test[y_test == not_n] = 1\n",
    "    \n",
    "    X = np.array(X_train)\n",
    "    y = np.array(y_train)\n",
    "    \n",
    "    # split train and validation\n",
    "    sss = StratifiedShuffleSplit(n_splits = 10, test_size = 0.5, random_state = 0)\n",
    "    for train_idx, val_idx in sss.split(X_train, y_train):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    \n",
    "    X_train, y_train = subsampling(X_train, y_train)\n",
    "    X_val, y_val = subsampling(X_val, y_val)\n",
    "    X_test, y_test = subsampling(X_test, y_test)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val , X_test, y_test\n",
    "\n",
    "def subsampling(X, y):\n",
    "    unique, count = np.unique(y, return_counts = True)\n",
    "    n_sampling = count[np.argmin(count)]\n",
    "    \n",
    "    idx_list = []\n",
    "    for i in range(len(unique)):\n",
    "        if count[i] <= n_sampling:\n",
    "            idx_list.append(np.where(y == unique[i])[0])\n",
    "            continue\n",
    "        idx = np.where(y == unique[i])[0]\n",
    "        idx = np.random.choice(idx, n_sampling, replace = False)\n",
    "        idx_list.append(idx)\n",
    "    \n",
    "    all_idx = np.sort(np.concatenate(idx_list))\n",
    "    X_sub = X[all_idx]\n",
    "    y_sub = y[all_idx]\n",
    "    \n",
    "    return X_sub, y_sub\n",
    "\n",
    "# Invoke the above function to get our data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_mnist_data(X_train, y_train, X_test, y_test)\n",
    "print('Train data shape: {}'.format(X_train.shape))\n",
    "print('Train label shape: {}'.format(y_train.shape))\n",
    "print('Validation data shape: {}'.format(X_val.shape))\n",
    "print('Validation label shape: {}'.format(y_val.shape))\n",
    "print('Test data shape: {}'.format(X_test.shape))\n",
    "print('Test label shape: {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First implement the naive softmax loss function with nested loops\n",
    "def softmax_loss_naive(W, X, y, reg):\n",
    "    \n",
    "  \"\"\"\n",
    "  Softmax loss function, naive implementation (with loops)\n",
    "\n",
    "  Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "  of N examples.\n",
    "\n",
    "  Inputs:\n",
    "  - W: A numpy array of shape (D, C) containing weights.\n",
    "  - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "  - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "    that X[i] has label c, where 0 <= c < C.\n",
    "  - reg: (float) regularization strength\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - loss as single float\n",
    "  - gradient with respect to weights W; an array of same shape as W\n",
    "  \"\"\"\n",
    "  # Initialize the loss and gradient to zero.\n",
    "  loss = 0.0\n",
    "  dW = np.zeros_like(W)\n",
    "\n",
    "  #############################################################################\n",
    "  # TODO: Compute the softmax loss and its gradient using explicit loops.     #\n",
    "  # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
    "  # here, it is easy to run into numeric instability. Don't forget the        #\n",
    "  # regularization!                                                           #\n",
    "  #############################################################################\n",
    "  N = X.shape[0]\n",
    "  D = X.shape[1]\n",
    "  C = W.shape[1]\n",
    "  data_loss = 0.0\n",
    "  \n",
    "  prob = np.zeros((N, C))\n",
    "  for i in range(N):\n",
    "    score = np.zeros((C))\n",
    "    for j in range(C):\n",
    "      score[j] = np.sum(X[i] * W[:, j])\n",
    "    exp_score = np.exp(score)\n",
    "    prob[i] = exp_score / np.sum(exp_score)\n",
    "  \n",
    "  for i in range(N):\n",
    "    correct_logprob = - np.log(prob[i, y[i]])\n",
    "    data_loss += correct_logprob\n",
    "  reg_loss = 0.0\n",
    "  for i in range(D):\n",
    "    for j in range(C):\n",
    "      reg_loss += W[i, j] ** 2\n",
    "  reg_loss *= 0.5 * reg\n",
    "  loss = data_loss / N + reg_loss\n",
    "  \n",
    "  dscore = np.array(prob)\n",
    "  for i in range(N):\n",
    "    dscore[i, y[i]] -= 1\n",
    "  # dscore[range(N), y] not dscore[:, y]\n",
    "  dscore /= N\n",
    "  for i in range(D):\n",
    "    for j in range(C):\n",
    "      dW[i, j] = np.sum(X[:, i] * dscore[:, j])\n",
    "      if i != D - 1:\n",
    "        dW[i, j] += reg * W[i, j]\n",
    "  #############################################################################\n",
    "  #                          END OF YOUR CODE                                 #\n",
    "  #############################################################################\n",
    "\n",
    "  return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.302628\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(785, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_train, y_train, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Since there are 10 classes, a randomly-initialized weight will predict each class with probability 0.1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 0.000057 analytic: 0.002742, relative error: 9.596021e-01\n",
      "numerical: 0.001428 analytic: -0.004319, relative error: 9.999983e-01\n",
      "numerical: 0.000038 analytic: -0.002282, relative error: 9.999957e-01\n",
      "numerical: 0.033055 analytic: 0.030680, relative error: 3.726030e-02\n",
      "numerical: 0.000013 analytic: 0.004838, relative error: 9.948409e-01\n",
      "numerical: 0.000789 analytic: 0.008150, relative error: 8.234574e-01\n",
      "numerical: 0.000003 analytic: -0.002346, relative error: 9.999957e-01\n",
      "numerical: 0.001808 analytic: 0.000977, relative error: 2.986024e-01\n",
      "numerical: 0.020913 analytic: 0.022593, relative error: 3.861060e-02\n",
      "numerical: -0.000207 analytic: -0.009691, relative error: 9.582122e-01\n",
      "numerical: 0.007670 analytic: 0.007670, relative error: 3.092886e-09\n",
      "numerical: -0.003231 analytic: -0.003231, relative error: 1.780195e-08\n",
      "numerical: 0.006271 analytic: 0.006271, relative error: 6.052792e-10\n",
      "numerical: 0.001502 analytic: 0.001502, relative error: 2.719138e-09\n",
      "numerical: 0.002243 analytic: 0.002243, relative error: 1.118889e-08\n",
      "numerical: -0.137623 analytic: -0.137623, relative error: 1.069009e-09\n",
      "numerical: 0.001404 analytic: 0.001404, relative error: 1.029020e-09\n",
      "numerical: -0.000886 analytic: -0.000886, relative error: 9.204192e-10\n",
      "numerical: 0.046128 analytic: 0.046128, relative error: 4.435037e-09\n",
      "numerical: -0.009318 analytic: -0.009318, relative error: 1.156941e-10\n"
     ]
    }
   ],
   "source": [
    "# Use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "def grad_check_sparse(f, x, analytic_grad, num_checks=10, h=1e-5):\n",
    "  \"\"\"\n",
    "  sample a few random elements and only return numerical\n",
    "  in this dimensions.\n",
    "  \"\"\"\n",
    "\n",
    "  for i in xrange(num_checks):\n",
    "    ix = tuple([randrange(m) for m in x.shape])\n",
    "\n",
    "    oldval = x[ix]\n",
    "    x[ix] = oldval + h # increment by h\n",
    "    fxph = f(x) # evaluate f(x + h)\n",
    "    x[ix] = oldval - h # increment by h\n",
    "    fxmh = f(x) # evaluate f(x - h)\n",
    "    x[ix] = oldval # reset\n",
    "\n",
    "    grad_numerical = (fxph - fxmh) / (2 * h)\n",
    "    grad_analytic = analytic_grad[ix]\n",
    "    rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic) + 10 ** -8)\n",
    "    print('numerical: %f analytic: %f, relative error: %e' % (grad_numerical, grad_analytic, rel_error))\n",
    "\n",
    "\n",
    "f = lambda w: softmax_loss_naive(w, X_train, y_train, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# Do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_train, y_train, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_train, y_train, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.302628e+00 computed in 1.488722s\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_train, y_train, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_loss_vectorized(W, X, y, reg):\n",
    "  \"\"\"\n",
    "  Softmax loss function, vectorized version.\n",
    "\n",
    "  Inputs and outputs are the same as softmax_loss_naive.\n",
    "  \"\"\"\n",
    "  # Initialize the loss and gradient to zero.\n",
    "  loss = 0.0\n",
    "  dW = np.zeros_like(W)\n",
    "\n",
    "  #############################################################################\n",
    "  # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
    "  # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
    "  # here, it is easy to run into numeric instability. Don't forget the        #\n",
    "  # regularization!                                                           #\n",
    "  #############################################################################\n",
    "  N = X.shape[0]\n",
    "  D = X.shape[1]\n",
    "  C = W.shape[1]\n",
    "  data_loss = 0.0\n",
    "  \n",
    "  scores = np.dot(X, W)\n",
    "  exp_scores = np.exp(scores)\n",
    "  probs = exp_scores / np.sum(exp_scores, axis = 1, keepdims = True) # keepdims: broadcast\n",
    "  correct_logprobs = - np.log(probs[range(N), y]) # range(N)\n",
    "  data_loss += np.sum(correct_logprobs) / N\n",
    "  \n",
    "  reg_loss = 0.5 * reg * np.sum(W * W)\n",
    "  loss = data_loss + reg_loss\n",
    "  \n",
    "  dscores = probs\n",
    "  dscores[range(N), y] -= 1\n",
    "  dscores /= N\n",
    "  dW = np.dot(X.T, dscores)\n",
    "  dW[:, :-1] += reg * W[:, :-1]\n",
    "  #############################################################################\n",
    "  #                          END OF YOUR CODE                                 #\n",
    "  #############################################################################\n",
    "\n",
    "  return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorized loss: 2.302628e+00 computed in 0.028544s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_train, y_train, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# We use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord = 'fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearClassifier(object):\n",
    "\n",
    "  def __init__(self):\n",
    "    self.W = None\n",
    "\n",
    "  def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100,\n",
    "            batch_size=200, verbose=False):\n",
    "    \"\"\"\n",
    "    Train this linear classifier using stochastic gradient descent.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) containing training data; there are N\n",
    "      training samples each of dimension D.\n",
    "    - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
    "      means that X[i] has label 0 <= c < C for C classes.\n",
    "    - learning_rate: (float) learning rate for optimization.\n",
    "    - reg: (float) regularization strength.\n",
    "    - num_iters: (integer) number of steps to take when optimizing\n",
    "    - batch_size: (integer) number of training examples to use at each step.\n",
    "    - verbose: (boolean) If true, print progress during optimization.\n",
    "\n",
    "    Outputs:\n",
    "    A list containing the value of the loss function at each training iteration.\n",
    "    \"\"\"\n",
    "    num_train, dim = X.shape\n",
    "    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
    "    if self.W is None:\n",
    "      # lazily initialize W\n",
    "      self.W = 0.001 * np.random.randn(dim, num_classes)\n",
    "\n",
    "    # Run stochastic gradient descent to optimize W\n",
    "    loss_history = []\n",
    "    for it in xrange(num_iters):\n",
    "      X_batch = None\n",
    "      y_batch = None\n",
    "\n",
    "      #########################################################################\n",
    "      # TODO:                                                                 #\n",
    "      # Sample batch_size elements from the training data and their           #\n",
    "      # corresponding labels to use in this round of gradient descent.        #\n",
    "      # Store the data in X_batch and their corresponding labels in           #\n",
    "      # y_batch; after sampling X_batch should have shape (dim, batch_size)   #\n",
    "      # and y_batch should have shape (batch_size,)                           #\n",
    "      #                                                                       #\n",
    "      # Hint: Use np.random.choice to generate indices. Sampling with         #\n",
    "      # replacement is faster than sampling without replacement.              #\n",
    "      #########################################################################\n",
    "      idx = np.random.choice(range(num_train), batch_size, replace = False)\n",
    "      X_batch = X[idx]\n",
    "      y_batch = y[idx]\n",
    "      #########################################################################\n",
    "      #                       END OF YOUR CODE                                #\n",
    "      #########################################################################\n",
    "\n",
    "      # evaluate loss and gradient\n",
    "      loss, grad = self.loss(X_batch, y_batch, reg)\n",
    "      loss_history.append(loss)\n",
    "\n",
    "      # perform parameter update\n",
    "      #########################################################################\n",
    "      # TODO:                                                                 #\n",
    "      # Update the weights using the gradient and the learning rate.          #\n",
    "      #########################################################################\n",
    "      self.W += - learning_rate * grad\n",
    "      #########################################################################\n",
    "      #                       END OF YOUR CODE                                #\n",
    "      #########################################################################\n",
    "\n",
    "      if verbose and it % 100 == 0:\n",
    "        print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "\n",
    "    return loss_history\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\"\n",
    "    Use the trained weights of this linear classifier to predict labels for\n",
    "    data points.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) containing training data; there are N\n",
    "      training samples each of dimension D.\n",
    "\n",
    "    Returns:\n",
    "    - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
    "      array of length N, and each element is an integer giving the predicted\n",
    "      class.\n",
    "    \"\"\"\n",
    "    y_pred = np.zeros(X.shape[0])\n",
    "    ###########################################################################\n",
    "    # TODO:                                                                   #\n",
    "    # Implement this method. Store the predicted labels in y_pred.            #\n",
    "    ###########################################################################\n",
    "    scores = np.dot(X, self.W)\n",
    "    y_pred = np.argmax(scores, axis = 1)\n",
    "    ###########################################################################\n",
    "    #                           END OF YOUR CODE                              #\n",
    "    ###########################################################################\n",
    "    return y_pred\n",
    "  \n",
    "  def loss(self, X_batch, y_batch, reg):\n",
    "    \"\"\"\n",
    "    Compute the loss function and its derivative. \n",
    "    Subclasses will override this.\n",
    "\n",
    "    Inputs:\n",
    "    - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n",
    "      data points; each point has dimension D.\n",
    "    - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
    "    - reg: (float) regularization strength.\n",
    "\n",
    "    Returns: A tuple containing:\n",
    "    - loss as a single float\n",
    "    - gradient with respect to self.W; an array of the same shape as W\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class Softmax(LinearClassifier):\n",
    "  \"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n",
    "\n",
    "  def loss(self, X_batch, y_batch, reg):\n",
    "    return softmax_loss_vectorized(self.W, X_batch, y_batch, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 1.000000e-06 reg 2.500000e+04 train accuracy: 0.408484 val accuracy: 0.410857\n",
      "lr 1.000000e-06 reg 5.000000e+04 train accuracy: 0.603975 val accuracy: 0.608721\n",
      "lr 5.000000e-06 reg 2.500000e+04 train accuracy: 0.937704 val accuracy: 0.935627\n",
      "lr 5.000000e-06 reg 5.000000e+04 train accuracy: 0.917384 val accuracy: 0.917532\n",
      "best validation accuracy achieved during cross-validation: 0.935627\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.6 on the validation set.\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-6, 5e-6]\n",
    "regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# save the best trained softmax classifer in best_softmax.                     #\n",
    "################################################################################\n",
    "for lr in learning_rates:\n",
    "    for rs in regularization_strengths:\n",
    "        clf = Softmax()\n",
    "        clf.train(X_train, y_train, lr, rs, num_iters = 1000, batch_size = 200)\n",
    "        y_pred_train = clf.predict(X_train)\n",
    "        train_acc = np.mean(y_pred_train == y_train)\n",
    "        y_pred = clf.predict(X_val)\n",
    "        val_acc = np.mean(y_pred == y_val)\n",
    "        if val_acc > best_val:\n",
    "          best_val = val_acc\n",
    "          best_softmax = clf\n",
    "        results[(lr, rs)] = train_acc, val_acc\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.941850\n"
     ]
    }
   ],
   "source": [
    "# evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADHCAYAAAAJSqg8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a62c790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(28, 28, 2)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['not {}'.format(not_n), '{}'.format(not_n)]\n",
    "for i in range(2):\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
